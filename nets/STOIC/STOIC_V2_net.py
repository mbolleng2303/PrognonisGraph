import math
import warnings

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from layers.SIMNet import SIMNet
import dgl
import wandb
import pandas as pd
"""
    GraphSAGE: 
    William L. Hamilton, Rex Ying, Jure Leskovec, Inductive Representation Learning on Large Graphs (NeurIPS 2017)
    https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf
"""

from layers.graphsage_layer import GraphSageLayer as GraphSageLayer
from layers.mlp_readout_layer import MLPReadout
from layers.preprocessing import Preprocessing
from train.train_STOIC import  compute_roc_auc
class Hyper(torch.nn.Module):
    def __init__(self, epsilon=1e-7):
        super().__init__()
        self.epsilon = epsilon

    def dist(self, u, v):
        sqdist = torch.sum((u - v) ** 2, dim=-1)
        squnorm = torch.sum(u ** 2, dim=-1)
        sqvnorm = torch.sum(v ** 2, dim=-1)
        x = 1 + 2 * sqdist / ((1 - squnorm) * (1 - sqvnorm)) + self.epsilon
        z = torch.sqrt(x ** 2 - 1)
        return torch.log(x + z)

    def forward(self, src, dst):

        return self.dist(src, dst)
def _trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    # Values are generated by using a truncated uniform distribution and
    # then using the inverse CDF for the normal distribution.
    # Get upper and lower cdf values
    l = norm_cdf((a - mean) / std)
    u = norm_cdf((b - mean) / std)

    # Uniformly fill tensor with values from [l, u], then translate to
    # [2l-1, 2u-1].
    tensor.uniform_(2 * l - 1, 2 * u - 1)

    # Use inverse cdf transform for normal distribution to get truncated
    # standard normal
    tensor.erfinv_()

    # Transform to proper mean, std
    tensor.mul_(std * math.sqrt(2.))
    tensor.add_(mean)

    # Clamp to ensure it's in the proper range
    tensor.clamp_(min=a, max=b)
    return tensor
def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    NOTE: this impl is similar to the PyTorch trunc_normal_, the bounds [a, b] are
    applied while sampling the normal with mean/std applied, therefore a, b args
    should be adjusted to match the range of mean, std args.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    with torch.no_grad():
        return _trunc_normal_(tensor, mean, std, a, b)

def symmetric_mse_loss(input1, input2):
    assert input1.size() == input2.size()
    feat = input1.size()[1]
    return torch.sum((input1 - input2) ** 2, dim=1) / feat


class STOIC_V2(nn.Module):
    def __init__(self, net_params, tresh=None):
        super().__init__()

        imaging_dim = 1024
        ehr_dim = 1
        n_classes = 2
        self.ehr_data = ['Sex', 'Age']
        feature_dim = net_params['feature_dim']
        similarity_dim = net_params['similarity_dim']
        mlp_dim = net_params['mlp_dim']
        if net_params['edge_feat']:
            self.layer_type = 'anisotropic'
        else:
            self.layer_type = 'isotropic'
        if net_params['cls_token']:
            self.cls_token = nn.Parameter(torch.zeros(1, 1, feature_dim))
            nn.init.normal_(self.cls_token, std=1e-6)
            self.norm_token = nn.LayerNorm(feature_dim)
            n_tokens = len(self.ehr_data)+2
        else:
            n_tokens = len(self.ehr_data)+1
        in_feat_dropout = net_params['in_feat_dropout']
        dropout = net_params['dropout']
        aggregator_type = net_params['sage_aggregator']
        n_layers = net_params['L']
        n_transformer = net_params['n_transformers']
        batch_norm = net_params['batch_norm']
        residual = net_params['residual']
        n_head = net_params['n_heads']
        self.net_params = net_params
        self.n_classes = n_classes
        self.device = net_params['device']
        self.embedding_ehr = nn.ModuleList([nn.Linear(ehr_dim, feature_dim) for _ in range(len(self.ehr_data))])
        self.norm_ehr = nn.ModuleList([nn.LayerNorm(feature_dim) for _ in range(len(self.ehr_data))])
        self.embedding_img = nn.Linear(imaging_dim, feature_dim)
        self.norm_img = nn.LayerNorm(feature_dim)
        self.project_sim = nn.Linear(feature_dim, similarity_dim)#, bias=False
        self.embedding_dist = nn.Linear(1, 1)
        self.embedding_e = nn.Linear(1, feature_dim)
        self.in_feat_dropout = nn.Dropout(in_feat_dropout)
        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_dim, nhead=n_head, dropout=0)
        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=n_transformer)
        self.layers = nn.ModuleList([GraphSageLayer(feature_dim, feature_dim, F.relu,
                                                    dropout, aggregator_type, batch_norm, residual) for _ in
                                     range(n_layers - 1)])
        self.layers.append(GraphSageLayer(feature_dim, mlp_dim, F.relu, dropout, aggregator_type, batch_norm, residual))
        self.MLP_layer = MLPReadout(mlp_dim, n_classes)
        self.MLP_layer_int = MLPReadout(feature_dim, n_classes)
        self.norm_feat = nn.LayerNorm(feature_dim)
        self.norm_sim = nn.LayerNorm(similarity_dim)
        self.norm_dist = nn.LayerNorm(400*400)

        self.pos_embed = nn.Parameter(torch.randn(n_tokens, 1, feature_dim) * .02)
        trunc_normal_(self.pos_embed, std=.02)
        self.param_vec = nn.Parameter(torch.zeros(1, feature_dim))
        nn.init.normal_(self.param_vec, std=1e-6)
        self.hyper = Hyper()

    def transformer_forward(self, g):
        img = g.ndata['feat']
        ehr = [np.reshape(g.ndata[i], (-1, 1)).float() for i in self.ehr_data]
        img = self.embedding_img(img.float())
        img = self.in_feat_dropout(self.norm_img(F.relu(img)))
        sequence = torch.unsqueeze(img, 0)
        # e = self.embedding_e(np.reshape(g.edata['feat'], (-1, 1)).float())

        for i in range(len(ehr)):
            ehr[i] = torch.unsqueeze(self.embedding_ehr[i](ehr[i].float()), 0)
            ehr[i] = self.in_feat_dropout(self.norm_ehr[i](F.gelu(ehr[i])))
            sequence = torch.cat((sequence, ehr[i]), dim=0)
        # Encoding
        if self.net_params['cls_token']:
            sequence = torch.cat((sequence, self.norm_token(F.gelu(self.cls_token.expand(-1, 400, -1)))), dim=0)
            encoding = self.transformer_encoder(sequence)
            encoding = encoding[-1, :, :]
        else:
            encoding = self.transformer_encoder(sequence)
            encoding = torch.mean(encoding, dim=0)

        encoding = self.norm_feat(encoding)
        self.score_int = self.MLP_layer_int(encoding)

        return encoding

    def apply_edge_processing(self, g):
        g.apply_edges(func=self.calc_dist)
        # mask = torch.arange(g.number_of_edges())[torch.squeeze(g.edata['mask'])].to(self.device)
        # transform = RemoveSelfLoop()
        # g.edata['similarity'][mask] = 0
        self.A = torch.reshape(g.edata['similarity'], (
        int(np.sqrt(g.edata['similarity'].size(0))), int(np.sqrt(g.edata['similarity'].size(0)))))
        # g = dgl.remove_edges(g, mask)
        return g

    def calc_dist(self, edges):
        self.Pi = edges.dst['h']
        self.Pj = edges.src['h']
        self.sim = torch.unsqueeze(self.hyper(self.Pi, self.Pj).float(), dim=1)  # 1-symmetric_mse_loss
        self.elab = torch.unsqueeze(torch.logical_xor(edges.dst['Label'], edges.src['Label']), dim=1).long()
        # sim = torch.relu(cosine(torch.relu(edges.src['h']*self.param_vec), torch.relu(edges.dst['h']), dim =1))
        # mask = self.sim <= 0.5
        return {'similarity': self.sim, 'label': self.elab}  # , 'mask': mask

    def representation_learning(self, encoded_feat, g):
        self.feat_sim = self.project_sim(encoded_feat)
        g.ndata['h'] = self.feat_sim
        # Graph representation learning
        g = self.apply_edge_processing(g)

        g = dgl.sampling.select_topk(g.to('cpu'), 90, 'similarity', output_device=self.device, ascending=False)
        self.sim_smp = g.edata['similarity']
        self.elab_smp = g.edata['label']
        e = g.edata['similarity'].expand(-1, encoded_feat.size(1))
        return g, e

    def forward_graph(self, encoded_feat, g, e):
        if self.layer_type == 'isotropic':
            h = encoded_feat
            for conv in self.layers:
                h = conv(g, h)
            # output
            self.h_out = self.MLP_layer(h)
            return self.h_out
        else:
            h = encoded_feat
            for conv in self.layers:
                h = conv(g, h, e)
            # output
            self.h_out = self.MLP_layer(h)
            return self.h_out

    def forward(self, g):
        # Embedding multi-modal feature
        encoded_feat = self.transformer_forward(g)
        #DF = pd.DataFrame(encoded_feat.detach().numpy())
        #DF.to_csv("extract_feat.csv")
        extracted_feat = encoded_feat.detach() #torch.tensor(np.array(pd.read_csv("extract_feat.csv"))[:,1:])
        g, e = self.representation_learning(extracted_feat, g)
        out = self.forward_graph(extracted_feat, g, e)
        return out, self.A, None

    def loss(self, pred, label):
        scores_severity = []
        labels_lst = []
        batch_scores = self.score_int
        batch_labels = label
        for i in range(len(batch_scores)):
            score_value = float(batch_scores[i][1].item())
            lab_value = int(torch.argmax(batch_labels[i]).item())
            if i == 0:
                scores_severity = np.expand_dims(np.array(score_value), axis=0)
                labels_lst = np.expand_dims(np.array(lab_value), axis=0)
            else:
                scores_severity = np.concatenate((scores_severity, np.expand_dims(np.array(score_value), axis=0)),
                                                 axis=0)
                labels_lst = np.concatenate((labels_lst, np.expand_dims(np.array(lab_value), axis=0)), axis=0)

        tr_acc = compute_roc_auc(labels=labels_lst, prediction=scores_severity)

        # weighted cross-entropy for unbalanced classes
        V = label.size(0)
        label_count = torch.bincount(label.long().argmax(dim=1))
        label_count = label_count[torch.nonzero(label_count, as_tuple=False)].squeeze()
        cluster_sizes = torch.zeros(self.n_classes).long().to(self.device)
        cluster_sizes[torch.unique(label)] = label_count
        weight = (V - cluster_sizes).float() / V
        weight *= (cluster_sizes > 0).float()
        criterion = torch.nn.CrossEntropyLoss(weight=weight)

        loss_gnn = criterion(self.h_out.float(), label.long().argmax(dim=1))#torch.tensor(0)#
        loss_tr = criterion(self.score_int.float(), label.long().argmax(dim=1))

        """N = self.A.size(0)

        d = torch.sum(self.A, dim=1)
        D = torch.diag(1/torch.sqrt(d))
        L = torch.eye(N) - torch.matmul(D, torch.matmul(self.A, D))
        smooth = nn.MSELoss()
        laplacian = torch.matmul(self.feat_sim.t(),
                     torch.matmul(L, self.feat_sim))
        n = self.feat_sim.size(1)
        loss_smooth = smooth(laplacian, torch.zeros_like(laplacian))# torch.tensor(0)#

        sparsity = nn.L1Loss()
        loss_sparsity = sparsity(self.A, torch.zeros_like(self.A)) #torch.tensor(0)#

        one = torch.ones(N, 1)
        loss_connectivity = -torch.matmul(one.t(), torch.log(torch.matmul(self.A, one)/N))/N#torch.tensor(0)#

        lambda_1 = 10
        lambda_2 = 0.2
        lambda_3 = 0.1
        lambda_4 = 1

        loss_grl = lambda_1*loss_sparsity + lambda_3*loss_smooth + lambda_2*loss_connectivity"""
        cosine_sim = nn.CosineEmbeddingLoss(margin=0)
        L2_loss = nn.L1Loss()
        contrast = L2_loss(self.sim, self.elab.float())  # cosine_sim(self.Pi, self.Pj, 2*self.elab-1)
        loss_grl = contrast  # lambda_1*loss_sparsity #+ lambda_3*loss_smooth #+ lambda_2*loss_connectivity

        contrast1 = L2_loss(self.sim_smp, self.elab_smp.float())
        loss_connectivity = contrast1
        """label = self.elab[:, 0]
        V = label.size(0)
        label_count = torch.bincount(label.long())
        label_count = label_count[torch.nonzero(label_count, as_tuple=False)].squeeze()
        cluster_sizes = torch.zeros(self.n_classes).long().to(self.device)
        cluster_sizes[torch.unique(label)] = label_count
        weight = (V - cluster_sizes).float() / V
        weight *= (cluster_sizes > 0).float()
        m = 1
        dist_loss = torch.sum(((1-label)/cluster_sizes[0])* symmetric_mse_loss(self.Pi, self.Pj)+(label)*(1/cluster_sizes[1])*torch.maximum(torch.zeros_like(label), m-symmetric_mse_loss(self.Pi, self.Pj)))"""
        criterion = nn.CosineEmbeddingLoss(margin=0.5)
        label = 2 * label - 1
        dist_loss = criterion(self.Pi, self.Pj, self.elab[:, 0])
        comb_loss = {'smooth': torch.tensor(0),
                     'sparsity': torch.tensor(0),
                     'connectivity': loss_connectivity,
                     'grl': loss_grl,
                     'transformer': loss_tr,
                     'gnn': loss_gnn,
                     'tr_acc': tr_acc}
        return loss_tr + loss_gnn + loss_connectivity, comb_loss

